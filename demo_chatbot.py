# pip install streamlit faiss-cpu sentence-transformers langchain transformers langchain_community

import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# C·∫•u h√¨nh Streamlit
st.set_page_config(page_title="FPT Chatbot", layout="centered")
st.title("ü§ñ Chatbot h·ªó tr·ª£ tuy·ªÉn sinh FPT Polytechnic")

# Load d·ªØ li·ªáu v√† t·∫°o vector store
@st.cache_resource
def load_retriever():
    with open("tuyensinh.txt", "r", encoding="utf-8") as f:
        raw_text = f.read()

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = splitter.split_text(raw_text)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    db = FAISS.from_texts(texts, embedding=embeddings)

    return db.as_retriever()

retriever = load_retriever()

# Load m√¥ h√¨nh sinh ti·∫øng Vi·ªát
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("VietAI/gpt-neo-1.3B-vietnamese-news")
    model = AutoModelForCausalLM.from_pretrained("VietAI/gpt-neo-1.3B-vietnamese-news")
    return tokenizer, model

tokenizer, model = load_model()

# H√†m sinh c√¢u tr·∫£ l·ªùi 
def generate_answer(query):
    docs = retriever.get_relevant_documents(query)

    if not docs:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

    context = "\n".join([doc.page_content for doc in docs[:3]])
    
    if not docs or len(docs) == 0:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

    prompt = f"""D·ª±a tr√™n ng·ªØ c·∫£nh sau, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ng·∫Øn g·ªçn v√† d·ªÖ hi·ªÉu:
            Ng·ªØ c·∫£nh:
            {context}

            C√¢u h·ªèi:
            {query}

            Tr·∫£ l·ªùi:"""

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
    outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=True,
            top_p=0.9,
            top_k=50,
            temperature=0.8,
            repetition_penalty=1.2,  # gi·∫£m l·∫∑p l·∫°i
            eos_token_id=tokenizer.eos_token_id  # d·ª´ng t·∫°i k·∫øt th√∫c c√¢u
            )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated = response[len(prompt):].strip()
    cleaned = generated.split("\n")[0].strip()
    return cleaned


# Giao di·ªán ng∆∞·ªùi d√πng
user_input = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")

if st.button("Tr·∫£ l·ªùi"):
    if user_input:
        with st.spinner("ƒêang truy xu·∫•t v√† sinh c√¢u tr·∫£ l·ªùi..."):
            answer = generate_answer(user_input)
            st.success("‚úÖ C√¢u tr·∫£ l·ªùi:")
            st.write(answer)
    else:
        st.warning("‚ùó Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")
